{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import queue\n",
    "import string\n",
    "import operator\n",
    "from multiprocessing import Process, Queue\n",
    "\n",
    "from robot import Robot\n",
    "from url_processor import UrlProcessor\n",
    "from mapreduce import MapReduce\n",
    "\n",
    "\n",
    "def save_structured():\n",
    "    pass  # TODO\n",
    "\n",
    "\n",
    "def do_job(pages_num, index, processed, to_do, another_to_do, letter, robot):\n",
    "    mapper = MapReduce()\n",
    "    while pages_num >= 0:\n",
    "        try:\n",
    "            url = to_do.get_nowait()\n",
    "        except queue.Empty:\n",
    "            break\n",
    "        else:\n",
    "            if UrlProcessor.get_name(url)[0] in letter:\n",
    "                if url in processed or not robot.is_allowed(url):\n",
    "                    continue\n",
    "\n",
    "                processed.append(url)\n",
    "\n",
    "                parsed_page = UrlProcessor.get_parsed_page(url)\n",
    "                paper_link = UrlProcessor.check_arxiv(url)\n",
    "                if not paper_link:\n",
    "                    word_counts = mapper([url])\n",
    "                    word_counts.sort(key=operator.itemgetter(1))\n",
    "                    word_counts.reverse()\n",
    "                    index[url] = word_counts\n",
    "                else:\n",
    "                    save_structured()\n",
    "\n",
    "                pages_num -= 1\n",
    "\n",
    "                for url_new in UrlProcessor.get_links(url, parsed_page):\n",
    "                    to_do.put(url_new)\n",
    "            else:\n",
    "                another_to_do.put(url)\n",
    "    return True\n",
    "\n",
    "\n",
    "init_pages = ['https://arxiv.org/', 'https://search.crossref.org/']\n",
    "pages_num = 2  # NOTE: for each process!\n",
    "\n",
    "alphabet = string.ascii_lowercase\n",
    "letter1, letter2 = alphabet[::4] + alphabet[1::4], alphabet[2::4] + alphabet[3::4]\n",
    "robot1, robot2 = Robot(), Robot()\n",
    "to_do1, to_do2 = Queue(), Queue()\n",
    "processed1, processed2 = [], []\n",
    "index1, index2 = {}, {}\n",
    "\n",
    "for page in init_pages:\n",
    "    if UrlProcessor.get_name(page)[0] in letter1:\n",
    "        to_do1.put(page)\n",
    "    else:\n",
    "        to_do2.put(page)\n",
    "\n",
    "processes = [\n",
    "    Process(target=do_job, args=[pages_num, index1, processed1, to_do1, to_do2, letter1, robot1]),\n",
    "    Process(target=do_job, args=[pages_num, index2, processed2, to_do2, to_do1, letter2, robot2])\n",
    "]\n",
    "\n",
    "for p in processes:\n",
    "    p.start()\n",
    "\n",
    "for p in processes:\n",
    "    p.join()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
