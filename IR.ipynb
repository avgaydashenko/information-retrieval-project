{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.stem import SnowballStemmer\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from bs4 import BeautifulSoup as bs\n",
    "\n",
    "from urllib.parse import urlparse, urljoin\n",
    "from urllib import parse\n",
    "from urllib.error import URLError\n",
    "import urllib.robotparser as urobot\n",
    "\n",
    "import string\n",
    "from copy import copy\n",
    "import requests\n",
    "import time\n",
    "import re\n",
    "import datetime\n",
    "import dateutil.parser as dparser\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "from shutil import copyfile\n",
    "\n",
    "import _pickle\n",
    "import hashlib\n",
    "import socket\n",
    "import email.utils as eutils\n",
    "import datetime\n",
    "\n",
    "from collections import deque\n",
    "import sys\n",
    "from shutil import copyfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class TextUtils: # TODO: cool name\n",
    "    @staticmethod\n",
    "    def text_to_words(text):\n",
    "        return word_tokenize(text.decode('utf-8').translate(str.maketrans(\"\", \"\", \"()!@#$%^&*_+=?<>~`',…©»\")))\n",
    "\n",
    "    @staticmethod\n",
    "    def filter_stop_words(words, locales):\n",
    "        current_words = words\n",
    "        for locale in locales:\n",
    "            current_words = [word for word in current_words if word not in stopwords.words(locale)]\n",
    "        return current_words\n",
    "\n",
    "    @staticmethod\n",
    "    def only_words(words):\n",
    "        return [word for word in words\n",
    "                if word != \"\" and\n",
    "                word[0] not in string.digits and\n",
    "                word[0] not in string.punctuation]\n",
    "\n",
    "    @staticmethod\n",
    "    def extract_places(text):\n",
    "        pass  # TODO using NLTK\n",
    "\n",
    "    @staticmethod\n",
    "    def stem(words, locale):\n",
    "        stemmer = SnowballStemmer(locale)\n",
    "        return [stemmer.stem(word) for word in words]\n",
    "\n",
    "    @staticmethod\n",
    "    def handle(text, main_locale, locales):\n",
    "        return TextUtils.stem(\n",
    "            TextUtils.only_words(TextUtils.filter_stop_words(\n",
    "                TextUtils.text_to_words(text),\n",
    "                locales=locales\n",
    "            )),\n",
    "            locale=main_locale\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Two-pass index\n",
    "\"\"\"\n",
    "\n",
    "class WordDocInfo:\n",
    "    def __init__(self, id, count, positions):\n",
    "        self.id = id\n",
    "        self.count = count\n",
    "        self.positions = positions\n",
    "\n",
    "    def add_position(self, pos):\n",
    "        self.count += 1\n",
    "        self.positions.append(pos)\n",
    "\n",
    "    def serialize(self):\n",
    "        return '{} {} {}\\n'.format(self.id, self.count, ' '.join(map(str, self.positions)))\n",
    "\n",
    "    @staticmethod\n",
    "    def deserialize(file):\n",
    "        word_doc_info = file.readline().decode('utf-8').split()\n",
    "        id = word_doc_info[0]\n",
    "        count = int(word_doc_info[1])\n",
    "        positions = list(map(int, word_doc_info[2:]))\n",
    "        return WordDocInfo(id, count, positions)\n",
    "\n",
    "\n",
    "class InvertedIndex:\n",
    "    def __init__(self):\n",
    "        self.file_name = None\n",
    "        self.words_begin = None\n",
    "\n",
    "    def create_index(self, docs, file_name='inverted_index.txt'):\n",
    "        self.file_name = file_name\n",
    "        words_size = {}\n",
    "        words_doc_count = {}\n",
    "        for num, doc_id in enumerate(docs):\n",
    "            cur_words = {}\n",
    "            text = docs[doc_id].get_text()\n",
    "            for pos, word in enumerate(text):\n",
    "                if word in cur_words:\n",
    "                    cur_words[word].add_position(pos)\n",
    "                else:\n",
    "                    cur_words[word] = WordDocInfo(doc_id, 1, [pos])\n",
    "            for word in cur_words:\n",
    "                if word not in words_doc_count:\n",
    "                    words_doc_count[word] = 0\n",
    "                if word not in words_size:\n",
    "                    words_size[word] = 0\n",
    "                words_size[word] += len(cur_words[word].serialize().encode('utf-8'))\n",
    "                words_doc_count[word] += 1\n",
    "            if num % 100 == 0:\n",
    "                print('indexing: passed {} from {}'.format(num, len(docs)))\n",
    "        for word in words_size:\n",
    "            words_size[word] += len('{} {}\\n'.format(word, words_doc_count[word]).encode('utf-8'))\n",
    "        self.words_begin = {}\n",
    "        cur_pos = 0\n",
    "        for word in words_size:\n",
    "            self.words_begin[word] = cur_pos\n",
    "            cur_pos += words_size[word]\n",
    "\n",
    "        cur_words_begin = copy(self.words_begin)\n",
    "        open(self.file_name, 'w').close() # Create if didn't exist & wipe\n",
    "        for word in cur_words_begin:\n",
    "            file = open(self.file_name, 'r+b')\n",
    "            file.seek(cur_words_begin[word])\n",
    "            file.write('{} {}\\n'.format(word, words_doc_count[word]).encode('utf-8'))\n",
    "            file.close()\n",
    "            cur_words_begin[word] += len('{} {}\\n'.format(word, words_doc_count[word]).encode('utf-8'))\n",
    "\n",
    "        for doc_id in docs:\n",
    "            cur_words = {}\n",
    "            text = docs[doc_id].get_text()\n",
    "            for pos, word in enumerate(text):\n",
    "                if word in cur_words:\n",
    "                    cur_words[word].add_position(pos)\n",
    "                else:\n",
    "                    cur_words[word] = WordDocInfo(doc_id, 1, [pos])\n",
    "            for word in cur_words:\n",
    "                file = open(self.file_name, 'r+b')\n",
    "                file.seek(cur_words_begin[word])\n",
    "                serialised_info = cur_words[word].serialize()\n",
    "                file.write(serialised_info.encode('utf-8'))\n",
    "                cur_words_begin[word] += len(serialised_info.encode('utf-8'))\n",
    "                file.close()\n",
    "\n",
    "    def get_index(self, word):\n",
    "        if word not in self.words_begin:\n",
    "            return None\n",
    "        else:\n",
    "            file = open(self.file_name, 'rb')\n",
    "            file.seek(self.words_begin[word])\n",
    "            cnt_docs = int(file.readline().decode('utf-8').split()[1])\n",
    "            info = []\n",
    "            for _ in range(cnt_docs):\n",
    "                info.append(WordDocInfo.deserialize(file))\n",
    "            file.close()\n",
    "            return info\n",
    "\n",
    "\n",
    "class Document:\n",
    "    def __init__(self, file_name):\n",
    "        self.file_name = file_name\n",
    "\n",
    "    def get_text(self):\n",
    "        with open(self.file_name, 'rb') as file_from:\n",
    "            text = file_from.read()\n",
    "            return TextUtils.handle(text, main_locale='russian', locales=['russian', 'english'])\n",
    "\n",
    "\n",
    "class TestDoc:\n",
    "    def __init__(self, text):\n",
    "        self.text = text\n",
    "\n",
    "    def get_text(self):\n",
    "        return self.text.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def is_absolute(url):\n",
    "    return bool(urlparse(url).netloc)\n",
    "\n",
    "\n",
    "class Page:\n",
    "    DEFAULT_WAIT_TIME = 1  # 1 second\n",
    "\n",
    "    def __init__(self, url, useragent, crawl_delay=None):\n",
    "        self.url = url\n",
    "        self.headers = {\n",
    "            'User-Agent': useragent\n",
    "        }\n",
    "        self.soup = None\n",
    "        self.crawl_delay = crawl_delay\n",
    "        self._page = None\n",
    "\n",
    "    def retrieve(self):\n",
    "        try:\n",
    "            self._page = requests.get(self.url, headers=self.headers)\n",
    "        except (requests.exceptions.MissingSchema, requests.exceptions.ConnectionError,\n",
    "                requests.exceptions.InvalidSchema, requests.exceptions.ChunkedEncodingError):\n",
    "            print(\"[PAGE -- retrieve] exception\")\n",
    "            return False\n",
    "        time_to_wait = self.DEFAULT_WAIT_TIME\n",
    "        if self.crawl_delay is not None \\\n",
    "                and self.DEFAULT_WAIT_TIME > self.crawl_delay:\n",
    "            time_to_wait = self.crawl_delay\n",
    "        status_code = self._page.status_code\n",
    "        time.sleep(time_to_wait)\n",
    "\n",
    "        if 400 <= status_code < 600:\n",
    "            # invalidate handler value due to an error\n",
    "            self.soup = None\n",
    "            return False\n",
    "        self.soup = bs(self._page.text, 'html.parser')\n",
    "        return True\n",
    "\n",
    "    def extract_urls(self, current_url):\n",
    "        result = []\n",
    "        if not self.allow_follow():\n",
    "            return result\n",
    "        for html_link in self.soup.find_all('a'):\n",
    "            link = html_link.get('href')\n",
    "            if is_absolute(link):\n",
    "                result.append(link)\n",
    "            else:\n",
    "                result.append(urljoin(current_url, link))\n",
    "        return result\n",
    "\n",
    "    def allow_cache(self):\n",
    "        return self.check_permission('NOARCHIVE')\n",
    "\n",
    "    def allow_index(self):\n",
    "        return self.check_permission('NOINDEX')\n",
    "\n",
    "    def allow_follow(self):\n",
    "        return self.check_permission('NOFOLLOW')\n",
    "\n",
    "    def check_permission(self, perm):\n",
    "        for tag in self.soup.find_all('ROBOTS', 'meta'):\n",
    "            if perm in tag['content'].split(', '):\n",
    "                return False\n",
    "        return True\n",
    "\n",
    "    def get_text(self):\n",
    "        if self.soup is None:\n",
    "            return \"\"\n",
    "        strings = []\n",
    "        for div in self.soup.find_all(['div', 'span', 'body']):\n",
    "            strings.extend([string for string in div.stripped_strings if string != \"\" and re.search(r'[<>{}=\\[\\]\\|]', string) is None])\n",
    "        return \" \".join(strings)\n",
    "\n",
    "    @staticmethod\n",
    "    def text_to_words(text):\n",
    "        return word_tokenize(text.translate(str.maketrans(\"\", \"\", \"()!@#$%^&*_+=?<>~`',…©»\")))\n",
    "\n",
    "    @staticmethod\n",
    "    def filter_stop_words(words, locale):\n",
    "        return [word for word in words if word not in stopwords.words(locale)]\n",
    "\n",
    "    @staticmethod\n",
    "    def only_words(words):\n",
    "        return [word for word in words\n",
    "                if word != \"\" and\n",
    "                word[0] not in string.digits and\n",
    "                word[0] not in string.punctuation]\n",
    "\n",
    "    def extract_places(self, text):\n",
    "        pass  # TODO using NLTK\n",
    "\n",
    "    @staticmethod\n",
    "    def stem(words, locale):\n",
    "        stemmer = SnowballStemmer(locale)\n",
    "        return [stemmer.stem(word) for word in words]\n",
    "\n",
    "def url_retriever_factory(url):\n",
    "    return URLBaseRetriever(url)\n",
    "\n",
    "\n",
    "class DBEntry:\n",
    "    def __init__(self, type, time, date, price, city, venue, name):\n",
    "        self.type = type\n",
    "        self.time = time\n",
    "        self.date = date\n",
    "        self.price = price\n",
    "        self.city = city\n",
    "        self.venue = venue\n",
    "        self.name = name\n",
    "\n",
    "\n",
    "class URLBaseRetriever:\n",
    "    def __init__(self, url, type=None, time=None, date=None, price=None, city=None, venue=None, name=None):\n",
    "        self.url = url\n",
    "\n",
    "        src = requests.get(url)\n",
    "        soup = bs(src.text, 'html.parser')\n",
    "        strings = []\n",
    "        for div in soup.find_all(['div', 'span', 'body']):\n",
    "            strings.extend([string for string in div.stripped_strings if string != \"\" and re.search(r'[<>{}=\\[\\]\\|]', string) is None])\n",
    "\n",
    "        self.time = time\n",
    "        self.date = date\n",
    "        if self.time is None or self.date is None:\n",
    "            for s in strings:\n",
    "                try:\n",
    "                    d = dparser.parse(s, fuzzy=True)\n",
    "                    if self.time is None:\n",
    "                        self.time = str(d.time())\n",
    "                    if self.date is None:\n",
    "                        self.date = str(d.date())\n",
    "                    break\n",
    "                except:\n",
    "                    continue\n",
    "\n",
    "        self.type = type\n",
    "        self.price = price\n",
    "        self.city = city\n",
    "        self.venue = venue\n",
    "\n",
    "        self.name = name\n",
    "        if self.name is None:\n",
    "            pieces = url.split('/')\n",
    "            self.name = pieces[len(pieces) - 1].split('?', 1)[0]\n",
    "\n",
    "    def get_type(self):\n",
    "        return self.type\n",
    "\n",
    "    def get_time(self):\n",
    "        return self.time\n",
    "\n",
    "    def get_date(self):\n",
    "        return self.date\n",
    "\n",
    "    def get_price(self):\n",
    "        return self.price\n",
    "\n",
    "    def get_city(self):\n",
    "        return self.city\n",
    "\n",
    "    def get_venue(self):\n",
    "        return self.venue\n",
    "\n",
    "    def get_name(self):\n",
    "        return self.name\n",
    "\n",
    "    def form_db_entry(self):\n",
    "        return DBEntry(self.get_type(), self.get_time(), self.get_date(), self.get_price(), self.get_city(), self.get_venue(), self.get_name())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crawler\n",
    "\n",
    "* Мы сохраняем файлы с именем == хеш от url сайта, так как сам url не атируется как название файла\n",
    "* Для того чтобы потом востановить название url через хеш -- мы храним отдельный файл с описанием: descr.txt\n",
    "* Каждые 100 итераций происходит сохранение накопившейся базы\n",
    "* Каждую 1000 итераций происходит "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Crawler:\n",
    "    USERAGENT = 'loaferspider'\n",
    "\n",
    "    def __init__(self, frontier, dir_to_save, dir_checkpoints, checkpoints_name, inv_index_file_name=\"inv.index\"):\n",
    "        self.dir_checkpoints = dir_checkpoints\n",
    "        self.frontier = frontier\n",
    "        self.dir_to_save = dir_to_save\n",
    "        self.documents = {}\n",
    "        self.file_description = 'descr.txt'\n",
    "        self.checkpoints_name = checkpoints_name\n",
    "        self.steps_count = 0\n",
    "        self.inv_index_file_name = inv_index_file_name\n",
    "        self.index = None\n",
    "\n",
    "    @staticmethod\n",
    "    def create_file_name(url_hash):\n",
    "        return 'document_from_url_with_hash_{}'.format(str(url_hash))\n",
    "\n",
    "    def restore(self):\n",
    "        with open(os.path.join(self.dir_checkpoints, self.file_description), 'r', encoding='utf-8') as file_descr:\n",
    "            for line in file_descr:\n",
    "                url, hash_value = line.strip().split('\\t')\n",
    "                path_to_file = os.path.join(self.dir_to_save, self.create_file_name(hash_value))\n",
    "                if os.path.exists(path_to_file):\n",
    "                    self.frontier.add_url(url)\n",
    "                    self.steps_count += 1\n",
    "                    self.documents[url] = path_to_file\n",
    "\n",
    "    def run(self):\n",
    "        while not self.frontier.done():\n",
    "            print(self.steps_count)\n",
    "            self.steps_count += 1\n",
    "            website = self.frontier.next_site()\n",
    "            if not website.read_robots_txt():\n",
    "                continue\n",
    "            current_url = website.next_url()\n",
    "            try:\n",
    "                website_delay = website.get_crawl_delay(self.USERAGENT)\n",
    "            except AttributeError as e:\n",
    "                print(\"[CRAWLER -- run] AttributeError: \", e)\n",
    "                continue\n",
    "            page = Page(current_url, self.USERAGENT, website_delay)\n",
    "            if not page.retrieve():\n",
    "                continue\n",
    "            if website.permit_crawl(current_url):\n",
    "                if page.allow_cache():\n",
    "                    text = page.get_text()\n",
    "                    self.store_document(current_url, text)\n",
    "                urls = page.extract_urls(current_url)\n",
    "                for url in urls:\n",
    "                    self.frontier.add_url(url)\n",
    "            if self.steps_count % 100 == 0:\n",
    "                self.create_checkpoint(self.steps_count)\n",
    "            self.frontier.releaseSite(website)\n",
    "            if self.steps_count % 10000 == 0:\n",
    "                self.create_index()\n",
    "\n",
    "    def store_document(self, url, text):\n",
    "        hash = hashlib.md5()\n",
    "        hash.update(url.encode('utf-8'))\n",
    "        hash_value = hash.hexdigest()\n",
    "        path = os.path.join(self.dir_to_save, self.create_file_name(hash_value))\n",
    "        with open(path, 'w', encoding='utf-8') as file_to:\n",
    "            print(text, file=file_to, end='')\n",
    "        self.documents[url] = path\n",
    "        return True\n",
    "\n",
    "    def create_checkpoint(self, count_passed):\n",
    "        try:\n",
    "            byte_present = pickle.dumps(self)\n",
    "        except _pickle.PicklingError:\n",
    "            print(\"Error getting pickling!\")\n",
    "            return\n",
    "        with open(os.path.join(self.dir_checkpoints, self.checkpoints_name), 'wb') as file:\n",
    "            file.write(byte_present)\n",
    "\n",
    "        with open(self.file_description, 'w', encoding='utf-8') as descr:\n",
    "            for url in self.documents:\n",
    "                print('{}\\t{}'.format(url, self.documents[url]), file=descr)\n",
    "        copyfile(self.file_description, os.path.join(self.dir_checkpoints, self.file_description))\n",
    "        print('Saved, step passed: {}, urls in queue: {}'.format(self.steps_count, self.frontier.cnt_added))\n",
    "\n",
    "    def create_index(self):\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sites \n",
    "Чтобы кроулить сайты нам нужно хранить следить за временем между кроулингом. Для этого для каждого сайта мы храним когда его опоследний раз кроулели.\n",
    "\n",
    "scheme -- название протокола\n",
    "\n",
    "hostname -- название домена в нижнем регистре"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Site:\n",
    "    _RESPONSE_TIMEOUT = 1\n",
    "\n",
    "    def __init__(self, scheme, hostname):\n",
    "        self.hostname = hostname\n",
    "        self.iter = 0\n",
    "        self.urls = {0: scheme + '://' + self.hostname}\n",
    "        self.timestamps = {0: None}\n",
    "        self._rp = urobot.RobotFileParser()\n",
    "        self._rp.set_url(scheme + '://' + hostname + '/robots.txt')\n",
    "\n",
    "    @staticmethod\n",
    "    def get_last_modified(url):\n",
    "        try:\n",
    "            r = requests.get(url)\n",
    "        except ConnectionError as err:\n",
    "            print(\"[SITE -- read_robots_txt] ConnectionError\")\n",
    "            return None\n",
    "        if 400 <= r.status_code < 600 or 'Last-Modified' not in r.headers:\n",
    "            return None\n",
    "        return datetime.datetime(*eutils.parsedate(r.headers['Last-Modified'])[:6])\n",
    "\n",
    "    def read_robots_txt(self):\n",
    "        default_timeout = socket.getdefaulttimeout()\n",
    "        socket.setdefaulttimeout(self._RESPONSE_TIMEOUT)\n",
    "        try:\n",
    "            self._rp.read()\n",
    "            status = True\n",
    "        except (URLError, ValueError) as e:\n",
    "            status = False\n",
    "            print(\"[SITE -- read_robots_txt] URL- or ValueError: \", e)\n",
    "        except socket.timeout:\n",
    "            status = False\n",
    "            print(\"[SITE -- read_robots_txt] socket.timeout\")\n",
    "        finally:\n",
    "            socket.setdefaulttimeout(default_timeout)\n",
    "        return status\n",
    "\n",
    "    def update_urls(self, url):\n",
    "        if url[-1] == '/':\n",
    "            url = url[:-1]\n",
    "        pattern = re.compile('\\.(css|jpg|pdf|docx|js|png|ico)$')\n",
    "        if pattern.search(url):\n",
    "            return False\n",
    "        if url not in self.urls.values():\n",
    "            cnt = len(self.urls)\n",
    "            self.urls[cnt] = url\n",
    "            self.timestamps[cnt] = None\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    # returns the first url which needs update (if any) or just hasn't been inspected yet\n",
    "    def next_url(self):\n",
    "        # setup\n",
    "        init_iter = self.iter\n",
    "        self.iter = (self.iter + 1) % len(self.urls)\n",
    "        url = self.urls[self.iter]\n",
    "        timestamp = self.timestamps[self.iter]\n",
    "        last_modified = self.get_last_modified(url)\n",
    "\n",
    "        # while we...\n",
    "        #   * haven't made the whole cycle\n",
    "        #   * and are sure that the page is up to date\n",
    "        #     (i.e. our timestamp for it is not None and is later or equal than its last-modified header)\n",
    "        # -- iterate\n",
    "        while self.iter != init_iter \\\n",
    "            and timestamp is not None and last_modified is not None \\\n",
    "            and timestamp >= last_modified:\n",
    "            self.iter = (self.iter + 1) % len(self.urls)\n",
    "            url = self.urls[self.iter]\n",
    "            timestamp = self.timestamps[self.iter]\n",
    "            last_modified = self.get_last_modified(url)\n",
    "        return url\n",
    "\n",
    "    def permit_crawl(self, url):\n",
    "        return self._rp.can_fetch('bot', url)\n",
    "\n",
    "    def get_crawl_delay(self, useragent):\n",
    "        try:\n",
    "            return self._rp.crawl_delay(useragent)\n",
    "        except AttributeError as e:\n",
    "            print(\"[SITE -- get_crawl_delay] url: {0} AttributeError:\".format(self.hostname), e)\n",
    "            return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UrlQueue\n",
    "This queue contain Sites \n",
    "\n",
    "Проверяет на существование такого сайта, и добовляет/выдает его из очереди. Храним сами сайты, а не url чтобы следить за временем последнего кроуленга сата."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class UrlQueue:\n",
    "    def __init__(self):\n",
    "        self.sites = {}\n",
    "        self.site_queue = deque()\n",
    "\n",
    "    def add_url(self, url):\n",
    "        parsed_url = parse.urlparse(url)\n",
    "        hostname = parsed_url.hostname\n",
    "        scheme = parsed_url.scheme\n",
    "        if scheme is None:\n",
    "            print(url)\n",
    "        if hostname not in self.sites.keys():\n",
    "            if hostname and scheme:\n",
    "                site = Site(scheme, hostname)\n",
    "                self.sites[hostname] = site\n",
    "            else:\n",
    "                return False\n",
    "        else:\n",
    "            site = self.sites[hostname]\n",
    "\n",
    "        self.site_queue.append(site)\n",
    "\n",
    "        return site.update_urls(url)\n",
    "\n",
    "    def next_site(self):\n",
    "        site = self.site_queue.popleft()\n",
    "        self.site_queue.append(site)\n",
    "        return site\n",
    "\n",
    "    def release_site(self, site):\n",
    "        self.site_queue.append(site)\n",
    "\n",
    "    def has_next_site(self):\n",
    "        return len(self.site_queue) != 0\n",
    "\n",
    "    def add_url_if_site_exists(self, url):\n",
    "        parsed_url = parse.urlparse(url)\n",
    "        hostname = parsed_url.hostname\n",
    "        if hostname in self.sites.keys():\n",
    "            site = self.sites[hostname]\n",
    "            self.site_queue.append(site)\n",
    "            return site.update_urls(url)\n",
    "        return False\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frontier\n",
    "It's same sort of Wrapper over Queue in wich we contain all urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Frontier:\n",
    "    def __init__(self, seed_urls, docs_bound):\n",
    "        self.cnt_added = 0\n",
    "        for seed_url in seed_urls:\n",
    "            self.queue = UrlQueue()\n",
    "            self.queue.add_url(seed_url)\n",
    "            self.docs_bound = docs_bound\n",
    "            self.cnt_added +=1\n",
    "\n",
    "    def done(self):\n",
    "        return not self.queue.has_next_site()\n",
    "\n",
    "    def next_site(self):\n",
    "        return self.queue.next_site()\n",
    "\n",
    "    def add_url(self, url):\n",
    "        if self.cnt_added < self.docs_bound:\n",
    "            self.cnt_added += 1\n",
    "            self.queue.add_url(url)\n",
    "        elif self.cnt_added < self.docs_bound*2:\n",
    "           if self.queue.add_url_if_site_exists(url):\n",
    "               self.cnt_added += 1\n",
    "\n",
    "    def releaseSite(self, site):\n",
    "        self.queue.release_site(site)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Crawler\n",
    "Create dirictory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_for_docs = 'documents'\n",
    "if not os.path.exists(dir_for_docs):\n",
    "    os.mkdir(dir_for_docs)\n",
    "\n",
    "dir_checkpoints = 'checkpoints'\n",
    "if not os.path.exists(dir_checkpoints):\n",
    "    os.mkdir(dir_checkpoints)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Const define"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "checkpoints_name = 'checkpoints.save'\n",
    "pages_bound = 10000\n",
    "seeds = ['https://search.crossref.org', 'https://arxiv.org']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define Crawler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No checkpoints were found.\n"
     ]
    }
   ],
   "source": [
    "if not os.listdir(dir_checkpoints):\n",
    "    print('No checkpoints were found.')\n",
    "    frontier = Frontier(seeds, pages_bound)\n",
    "    crawler = Crawler(frontier, dir_for_docs, dir_checkpoints, checkpoints_name)\n",
    "    if os.path.exists(os.path.join(dir_checkpoints, crawler.file_description)):\n",
    "        copyfile(os.path.join(dir_checkpoints, crawler.file_description), crawler.file_description)\n",
    "    else:\n",
    "        open(crawler.file_description, 'w').close() # Wipe file\n",
    "else:\n",
    "    with open(os.path.join(dir_checkpoints, checkpoints_name), 'rb') as check_file:\n",
    "        crawler_loaded = pickle.load(check_file)\n",
    "\n",
    "    frontier = Frontier(seeds, pages_bound)\n",
    "    crawler = Crawler(frontier, dir_for_docs, dir_checkpoints, checkpoints_name)\n",
    "\n",
    "    if (crawler_loaded.__dict__.keys() == crawler.__dict__.keys()) \\\n",
    "            and (crawler_loaded.frontier.__dict__.keys() == crawler.frontier.__dict__.keys()):\n",
    "        crawler = crawler_loaded\n",
    "        print('Found checkpoints. Loaded crawler. Count urls in queue is {}'.format(crawler.frontier.cnt_added))\n",
    "    else:\n",
    "        print('Found checkpoints. Unable to load crawler. Load sources.')\n",
    "        if os.path.exists(os.path.join(dir_checkpoints, crawler.file_description)):\n",
    "            copyfile(os.path.join(dir_checkpoints, crawler.file_description), crawler.file_description)\n",
    "        else:\n",
    "            open(crawler.file_description, 'w').close() # Wipe file\n",
    "        crawler.restore()\n",
    "        print(crawler.steps_count)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run Crawler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "[SITE -- read_robots_txt] URL- or ValueError:  <urlopen error _ssl.c:761: The handshake operation timed out>\n",
      "7\n",
      "[SITE -- read_robots_txt] URL- or ValueError:  <urlopen error _ssl.c:761: The handshake operation timed out>\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "[SITE -- read_robots_txt] socket.timeout\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "[SITE -- read_robots_txt] URL- or ValueError:  <urlopen error _ssl.c:761: The handshake operation timed out>\n",
      "141\n",
      "[SITE -- read_robots_txt] URL- or ValueError:  <urlopen error _ssl.c:761: The handshake operation timed out>\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n"
     ]
    },
    {
     "ename": "ConnectionError",
     "evalue": "('Connection aborted.', ConnectionResetError(54, 'Connection reset by peer'))",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mConnectionResetError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    600\u001b[0m                                                   \u001b[0mbody\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 601\u001b[0;31m                                                   chunked=chunked)\n\u001b[0m\u001b[1;32m    602\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    386\u001b[0m                     \u001b[0;31m# otherwise it looks like a programming error was the cause.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 387\u001b[0;31m                     \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    388\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mSocketTimeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBaseSSLError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSocketError\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/urllib3/packages/six.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    382\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 383\u001b[0;31m                     \u001b[0mhttplib_response\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetresponse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    384\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/http/client.py\u001b[0m in \u001b[0;36mgetresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1330\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1331\u001b[0;31m                 \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbegin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1332\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mConnectionError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/http/client.py\u001b[0m in \u001b[0;36mbegin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    296\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 297\u001b[0;31m             \u001b[0mversion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreason\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    298\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mstatus\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mCONTINUE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/http/client.py\u001b[0m in \u001b[0;36m_read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    257\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_read_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 258\u001b[0;31m         \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_MAXLINE\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"iso-8859-1\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    259\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0m_MAXLINE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    585\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mConnectionResetError\u001b[0m: [Errno 54] Connection reset by peer",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mProtocolError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/requests/adapters.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    439\u001b[0m                     \u001b[0mretries\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_retries\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 440\u001b[0;31m                     \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    441\u001b[0m                 )\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    638\u001b[0m             retries = retries.increment(method, url, error=e, _pool=self,\n\u001b[0;32m--> 639\u001b[0;31m                                         _stacktrace=sys.exc_info()[2])\n\u001b[0m\u001b[1;32m    640\u001b[0m             \u001b[0mretries\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/urllib3/util/retry.py\u001b[0m in \u001b[0;36mincrement\u001b[0;34m(self, method, url, response, error, _pool, _stacktrace)\u001b[0m\n\u001b[1;32m    356\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mread\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mFalse\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_method_retryable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 357\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_stacktrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    358\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mread\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/urllib3/packages/six.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(tp, value, tb)\u001b[0m\n\u001b[1;32m    684\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 685\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    686\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    600\u001b[0m                                                   \u001b[0mbody\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 601\u001b[0;31m                                                   chunked=chunked)\n\u001b[0m\u001b[1;32m    602\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    386\u001b[0m                     \u001b[0;31m# otherwise it looks like a programming error was the cause.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 387\u001b[0;31m                     \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    388\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mSocketTimeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBaseSSLError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSocketError\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/urllib3/packages/six.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    382\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 383\u001b[0;31m                     \u001b[0mhttplib_response\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetresponse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    384\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/http/client.py\u001b[0m in \u001b[0;36mgetresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1330\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1331\u001b[0;31m                 \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbegin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1332\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mConnectionError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/http/client.py\u001b[0m in \u001b[0;36mbegin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    296\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 297\u001b[0;31m             \u001b[0mversion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreason\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    298\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mstatus\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mCONTINUE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/http/client.py\u001b[0m in \u001b[0;36m_read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    257\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_read_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 258\u001b[0;31m         \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_MAXLINE\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"iso-8859-1\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    259\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0m_MAXLINE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    585\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mProtocolError\u001b[0m: ('Connection aborted.', ConnectionResetError(54, 'Connection reset by peer'))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mConnectionError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-671355ffced7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcrawler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-5-dba68941124b>\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     34\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mwebsite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_robots_txt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m                 \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m             \u001b[0mcurrent_url\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwebsite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext_url\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m                 \u001b[0mwebsite_delay\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwebsite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_crawl_delay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUSERAGENT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-50d3181f319f>\u001b[0m in \u001b[0;36mnext_url\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0murl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murls\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0mtimestamp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimestamps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m         \u001b[0mlast_modified\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_last_modified\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0;31m# while we...\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-50d3181f319f>\u001b[0m in \u001b[0;36mget_last_modified\u001b[0;34m(url)\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_last_modified\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m             \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mConnectionError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"[SITE -- read_robots_txt] ConnectionError\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/requests/api.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(url, params, **kwargs)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetdefault\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'allow_redirects'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'get'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/requests/api.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0;31m# cases, and look like a memory leak in others.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0msessions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/requests/sessions.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    506\u001b[0m         }\n\u001b[1;32m    507\u001b[0m         \u001b[0msend_kwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msettings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 508\u001b[0;31m         \u001b[0mresp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0msend_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    509\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    510\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/requests/sessions.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    616\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    617\u001b[0m         \u001b[0;31m# Send the request\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 618\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madapter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    619\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    620\u001b[0m         \u001b[0;31m# Total elapsed time of the request (approximately)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/requests/adapters.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    488\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mProtocolError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 490\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mConnectionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    491\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mMaxRetryError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mConnectionError\u001b[0m: ('Connection aborted.', ConnectionResetError(54, 'Connection reset by peer'))"
     ]
    }
   ],
   "source": [
    "crawler.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
