{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from multiprocessing import Lock, Process, Queue, current_process\n",
    "import time\n",
    "import queue\n",
    "import string\n",
    "import requests\n",
    "from lxml import html\n",
    "from robots import Robot\n",
    "import pickle\n",
    "MAX_NUM = 60000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_parsed_page(url):\n",
    "    return html.fromstring(requests.get(url).content)\n",
    "\n",
    "def get_links(parsed_page, url, url_doc, robot):\n",
    "    result = []\n",
    "    post_urls = parsed_page.xpath('//a/@href')\n",
    "    for post_url in post_urls:\n",
    "        if len(post_url) == 0:\n",
    "            continue\n",
    "        if post_url[0] == '/':\n",
    "            post_url = url + (post_url[1:] if post_url[-1] == '/' else post_url)\n",
    "        if post_url not in url_doc and robot.is_allowed(post_url):\n",
    "            result.append(post_url)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_env(url):\n",
    "    robot = Robot()\n",
    "    url_doc = {}\n",
    "    to_do = Queue()\n",
    "    parsed_page = get_parsed_page(url)\n",
    "    url_doc[url] = parsed_page\n",
    "    for url_new in get_links(parsed_page, url, url_doc, robot):\n",
    "        to_do.put(url_new)\n",
    "\n",
    "    return to_do, url_doc, robot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_job(to_do, todo_neib, url_doc, letter, robot, num_processes):\n",
    "    while True:\n",
    "        try:\n",
    "            url = to_do.get_nowait()\n",
    "        except queue.Empty:\n",
    "            break\n",
    "        else:\n",
    "            if url[8] in letter:\n",
    "                print(len(url_doc))\n",
    "                if len(url_doc) >= MAX_NUM:\n",
    "                    break\n",
    "\n",
    "                if url in url_doc or not robot.is_allowed(url):\n",
    "                    continue\n",
    "\n",
    "                parsed_page = get_parsed_page(url)\n",
    "                url_doc[url] = parsed_page\n",
    "                for url_new in get_links(parsed_page, url, url_doc, robot):\n",
    "                    to_do.put(url_new)\n",
    "                if len(url_doc) % 10 == 0:\n",
    "                    print(len(url_doc), num_processes)\n",
    "                    with open('filename'+str(num_processes)+'_'+str(len(url_doc)) +'.pickle', 'wb') as handle:\n",
    "                        pickle.dump(url_doc, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "            else:\n",
    "                todo_neib.put(url)\n",
    "    return True          \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process Process-80:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/malysheva/miniconda3/lib/python3.6/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/malysheva/miniconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"<ipython-input-94-cc7710bf9b38>\", line 23, in do_job\n",
      "    pickle.dump(url_doc, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
      "TypeError: can't pickle HtmlElement objects\n"
     ]
    }
   ],
   "source": [
    "number_of_processes = 2\n",
    "alphabet = string.ascii_lowercase\n",
    "processes = []\n",
    "\n",
    "\n",
    "to_do1, url_doc1, robot1 = create_env('https://search.crossref.org/')\n",
    "to_do0, url_doc0, robot0 = create_env('http://plos.org/')\n",
    "z = int(len(alphabet)/2)\n",
    "\n",
    "letter0 = alphabet[:z]\n",
    "p = Process(target=do_job, args=(to_do0, to_do1, url_doc0, letter0, robot0, 0))\n",
    "processes.append(p)\n",
    "p.start()\n",
    "\n",
    "letter1 = alphabet[z:]\n",
    "p = Process(target=do_job, args=(to_do1, to_do0, url_doc1, letter1, robot1, 1))\n",
    "\n",
    "\n",
    "processes.append(p)\n",
    "p.start()\n",
    "\n",
    "# completing process\n",
    "for p in processes:\n",
    "    p.join()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
